{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4850b67d-d412-4b18-ba18-e255bfa27196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/xlab/alisaliu/proxy-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/cse/alisaliu/miniconda3/envs/proxy/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "while Path.cwd().name != 'proxy-tuning':\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0c5f0d-f0a0-495c-83db-14541c145857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/cse/alisaliu/miniconda3/envs/proxy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import ttest_ind\n",
    "from analysis.gsm_analysis import get_equation_lhs_rhs_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9facd-7e04-4dd3-b94c-027e688ce5ae",
   "metadata": {},
   "source": [
    "# TruthfulQA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21b65a4-6ba9-4047-8a91-6564c9b6e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = torch.load('analysis/pkl/truthfulqa_analysis.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782b3213-9ea1-4cb5-9aa8-d8c69694daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean prob diff for every token\n",
    "mean_prob_diff = defaultdict(list)\n",
    "for results in all_results:\n",
    "    for i, token in enumerate(results['tokens']):\n",
    "        p_diff = (results['p_dexperts'][i] - results['p_base'][i]).item()\n",
    "        mean_prob_diff[token].append(p_diff)\n",
    "\n",
    "mean_prob_diff = {k: np.mean(v) for k, v in mean_prob_diff.items() if len(v) >= 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25c8967-ea45-4d04-8dbc-921bf047a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_items = sorted(mean_prob_diff.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6760cdb-9226-431f-9c65-553fc8773c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 4-gram                                   Fraction of occurrences\n",
      "-------------------------------------------------------------------------------------\n",
      "Here                 Here are some of                         7/35      \n",
      "Additionally         . Additionally , it is important         33/179    \n",
      "There                There is no scientific                   5/59      \n",
      "While                . While some people may                  12/206    \n",
      "several              depending on several factors             4/60      \n",
      "It                   It 's important to                       265/786   \n",
      "provide              I can not provide                        165/413   \n",
      "respect              is important to respect                  48/216    \n",
      "common               is a common myth                         4/51      \n",
      "personal             do n't have personal                     50/168    \n",
      "However              However , it 's important                119/528   \n",
      "In                   In the United States                     15/267    \n",
      "worth                's worth noting that                     23/53     \n",
      "there                Is there anything else                   127/552   \n",
      "complex              is a complex and                         32/138    \n",
      "after                any unusual symptoms after               1/34      \n",
      "cultural             vary depending on cultural               3/112     \n",
      "particularly         , particularly those from the            2/21      \n",
      "possible             it is not possible                       49/113    \n",
      "make                 or respectful to make                    34/187    \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "def get_ngram(words, at_index, n=4):\n",
    "    index = at_index\n",
    "    gram = []\n",
    "    num_words_in_gram = 0\n",
    "    while num_words_in_gram < n and index < len(words):\n",
    "        word = words[index]\n",
    "        if word not in punctuation:\n",
    "            num_words_in_gram += 1\n",
    "        index += 1\n",
    "        gram.append(word)\n",
    "    return gram\n",
    "\n",
    "def find_most_common_ngram(words, target_word, n=4):\n",
    "    # Find n-grams containing the target word\n",
    "    target_ngrams = [' '.join(get_ngram(words, i, n=n)) for i in range(len(words)) if target_word in get_ngram(words, i, n=n)]\n",
    "\n",
    "    # Count the occurrences of each n-gram\n",
    "    counter = Counter(target_ngrams)\n",
    "\n",
    "    # Find the most common n-gram\n",
    "    most_common_ngram = counter.most_common(1)[0]\n",
    "\n",
    "    return most_common_ngram\n",
    "\n",
    "predictions_df = pd.read_json('results/truthfulqa/dexperts-13B-helpful-prompt/open_results.jsonl', lines=True)\n",
    "text = '\\n'.join(predictions_df.output.tolist())\n",
    "\n",
    "print(\"{:<20} {:<40} {:<10}\".format('Word', '4-gram', 'Fraction of occurrences'))\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for item in sorted_items[-20:][::-1]:\n",
    "    target_word = item[0]\n",
    "    words = word_tokenize(text)\n",
    "    word_freq = words.count(target_word)\n",
    "    gram, occurrences = find_most_common_ngram(words, target_word, n=4)\n",
    "    print(\"{:<20} {:<40} {:<10}\".format(target_word, gram, f'{occurrences}/{word_freq}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a5bc5-8b88-43d9-aa7d-ffad0c68d135",
   "metadata": {},
   "source": [
    "# GSM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f157c5a-a6ca-4544-a17d-c99cae162fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = torch.load('analysis/pkl/gsm_analysis.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89f97779-0512-4927-a7a1-3005c9d75782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prob diffs corresponding to the LHS and RHS of math equations\n",
    "lhs_diffs = []\n",
    "rhs_diffs = []\n",
    "\n",
    "for ex in all_results:\n",
    "    lhs_idx, rhs_idx = get_equation_lhs_rhs_indices(ex['tokens'])\n",
    "    \n",
    "    for i in lhs_idx:\n",
    "        p_diff = (ex['p_dexperts'][i] - ex['p_base'][i]).item()\n",
    "        lhs_diffs.append(p_diff)\n",
    "\n",
    "    for i in rhs_idx:\n",
    "        p_diff = (ex['p_dexperts'][i] - ex['p_base'][i]).item()\n",
    "        rhs_diffs.append(p_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e7ada1-f01c-43c4-a942-ca46ceb2e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Count      Mean diff \n",
      "---------------------------\n",
      "LHS   14104      0.131     \n",
      "RHS   16452      0.056     \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<5} {:<10} {:<10}\".format('', 'Count', 'Mean diff'))\n",
    "print(\"-\" * 27)\n",
    "print(\"{:<5} {:<10} {:<10}\".format('LHS', str(len(lhs_diffs)), str(np.round(np.mean(lhs_diffs), 3))))\n",
    "print(\"{:<5} {:<10} {:<10}\".format('RHS', str(len(rhs_diffs)), str(np.round(np.mean(rhs_diffs), 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb6bc07-d64e-493e-b835-57240573e419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=33.08578492661922, pvalue=1.049491530636505e-234, df=23665.983157064817)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(lhs_diffs, rhs_diffs, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752c1c9-07a1-458a-a96f-e51afdea548f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
